{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports and requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для моделирования нам понадобится реализация градиентного бустинга из `lightgbm`. Для обработки данных &ndash; стандартный DS стек из `pandas`, `numpy` и `sklearn`. Так как мы будем использовать достаточно объемные выборки данных (кредитные истории клиентов), то нужно будет эффективно читать данные и выделять из них признаки, чтобы иметь возможность строить решение даже на локальных машинах с ограничениями по оперативной памяти.  Поэтому нам понадобятся инструменты для работы с форматом данных `Parquet`. Библиотека `pandas` представляет необходимый инструментарий (рекомендуем установить модуль `fastparquet`, это позволит еще быстрее считывать данные)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# если у вас есть CUDA, то она понадобится там для экспериментов в catboost\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* В данном соревновании участникам предстоит работать с таким форматом данных, как `Parquet`. Узнать подробнее об этом формате Вы можете самостоятельно. Самое главное &ndash; это крайне эффективный бинарный формат сжатия данных по колонокам. Однако, для непосредственной работы с данными и построения моделей нам нужно прочитать их и трансформировать в pandas.DataFrame. При этом сделать это эффективно по памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"../data/train_data/\"\n",
    "TEST_DATA_PATH = \"../data/test_data/\"\n",
    "\n",
    "TRAIN_TARGET_PATH = \"../data/train_target.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_parquet_dataset_from_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для примера прочитаем одну партицию в память и оценим, сколько RAM занимает весь DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6adf2708844862ae561ad84002b558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объем памяти в RAM одной партиции данных с кредитными историями: 0.964 GB\n",
      "Ожидаемый размер в RAM всего датасета: 11.564 GB\n"
     ]
    }
   ],
   "source": [
    "data_frame = read_parquet_dataset_from_local(TRAIN_DATA_PATH, start_from=0, num_parts_to_read=1)\n",
    "\n",
    "memory_usage_of_frame = data_frame.memory_usage(index=True).sum() / 10**9\n",
    "expected_memory_usage = memory_usage_of_frame * 12\n",
    "print(f\"Объем памяти в RAM одной партиции данных с кредитными историями: {round(memory_usage_of_frame, 3)} GB\")\n",
    "print(f\"Ожидаемый размер в RAM всего датасета: {round(expected_memory_usage, 3)} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Итого, при чтении всех данных сразу, они займут значительный объем памяти. Решение &ndash; читать данные итеративно небольшими чанками. Чанки организованы таким образом, что для конкретного клиента вся информация о его кредитной истории до момента подачи заявки на кредит расположена внутри одного чанка. Это позволяет загружать данные в память небольшими порциями, выделять все необходимые признаки и получать результирующий фрейм для моделирования. Для этих целей мы предоставляем вам функцию `utils.read_parquet_dataset_from_local`.\n",
    "\n",
    "Так же все чанки данных упорядочены по возрастанию даты заявки на кредит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data_frame\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* В рамках данного соревнования участникам предстоит работать с кредитной историей клиентов. На основе кредитной истории клиента до момента подачи заявки на новый кредит нужно оценить, насколько благонадежным является клиент, и определить вероятность его ухода в дефолт по новому кредиту. Каждый кредит описывается набором из 60 категориальных признаков.\n",
    "\n",
    "\n",
    "* Базовым подходом к решению этой задачи является построение классических моделей машинного обучения на аггрегациях от последовательностей категориальных признаков. В данном случае мы закодируем признаки с помощью one-hot-encoding'а, применим к ним count-аггрегирование и обучим на этом градиентный бустинг из реализации `lightgbm`.\n",
    "     \n",
    "     \n",
    " * Описанный подход к обработке кредитных историй клиентов реализован в виде класс-трансформера `CountAggregator` ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FEATURES_PATH = \"../data/train_features_gb/\"\n",
    "TEST_FEATURES_PATH = \"../data/test_features_gb/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r TRAIN_FEATURES_PATH\n",
    "! mkdir TRAIN_FEATURES_PATH\n",
    "! rm -r TEST_FEATURES_PATH\n",
    "! mkdir TEST_FEATURES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountAggregator(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.encoded_features = None\n",
    "        \n",
    "    def __extract_count_aggregations(self, data_frame: pd.DataFrame, mode: str) -> pd.DataFrame:\n",
    "        # one-hot-encoding\n",
    "        feature_columns = list(data_frame.columns.values)\n",
    "        feature_columns.remove(\"id\")\n",
    "        feature_columns.remove(\"rn\")\n",
    "\n",
    "        dummies = pd.get_dummies(data_frame[feature_columns], columns=feature_columns)\n",
    "        dummy_features = dummies.columns.values\n",
    "        \n",
    "        ohe_features = pd.concat([data_frame, dummies], axis=1)\n",
    "        ohe_features = ohe_features.drop(columns=feature_columns)\n",
    "        \n",
    "        # count aggregation\n",
    "        ohe_features.groupby(\"id\")\n",
    "        features = ohe_features.groupby(\"id\")[dummy_features].sum().reset_index(drop=False)\n",
    "        return features\n",
    "        \n",
    "    def __transform_data(self, path_to_dataset: str, num_parts_to_preprocess_at_once: int = 1, num_parts_total: int=50,\n",
    "                                     mode: str = \"fit_transform\", save_to_path=None, verbose: bool=False):\n",
    "        assert mode in [\"fit_transform\", \"transform\"], f\"Unrecognized mode: {mode}! Please use one of the following modes: \\\"fit_transform\\\", \\\"transform\\\"\"\n",
    "        preprocessed_frames = []\n",
    "        for step in tqdm.tqdm_notebook(range(0, num_parts_total, num_parts_to_preprocess_at_once), \n",
    "                                       desc=\"Transforming sequential credit data\"):\n",
    "            data_frame = read_parquet_dataset_from_local(path_to_dataset, start_from=step, \n",
    "                                                         num_parts_to_read=num_parts_to_preprocess_at_once, \n",
    "                                                         verbose=verbose)\n",
    "            features = self.__extract_count_aggregations(data_frame, mode=mode)\n",
    "            if save_to_path:\n",
    "                features.to_parquet(os.path.join(save_to_path, f\"processed_chunk_{step}.pq\"))\n",
    "            preprocessed_frames.append(features)\n",
    "        \n",
    "        features = pd.concat(preprocessed_frames)\n",
    "        features.fillna(np.uint8(0), inplace=True)\n",
    "        dummy_features = list(features.columns.values)\n",
    "        dummy_features.remove(\"id\")\n",
    "        if mode == \"fit_transform\":\n",
    "            self.encoded_features = dummy_features\n",
    "        else:\n",
    "            assert not self.encoded_features is None, \"Transformer not fitted\"\n",
    "            for col in self.encoded_features:\n",
    "                if not col in dummy_features:\n",
    "                    features[col] = np.uint8(0)\n",
    "        return features[[\"id\"]+self.encoded_features]\n",
    "    \n",
    "    def fit_transform(self, path_to_dataset: str, num_parts_to_preprocess_at_once: int = 1, num_parts_total: int = 50,\n",
    "                      save_to_path=None, verbose: bool=False):\n",
    "        return self.__transform_data(path_to_dataset=path_to_dataset,\n",
    "                                     num_parts_to_preprocess_at_once=num_parts_to_preprocess_at_once,\n",
    "                                     num_parts_total=num_parts_total, mode=\"fit_transform\",\n",
    "                                     save_to_path=save_to_path, verbose=verbose)\n",
    "    def transform(self, path_to_dataset: str, num_parts_to_preprocess_at_once: int = 1, num_parts_total: int=50,\n",
    "                  save_to_path=None, verbose: bool=False):\n",
    "        return self.__transform_data(path_to_dataset=path_to_dataset,\n",
    "                                     num_parts_to_preprocess_at_once=num_parts_to_preprocess_at_once,\n",
    "                                     num_parts_total=num_parts_total, mode=\"transform\",\n",
    "                                     save_to_path=save_to_path, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:28: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397e6413a6b4477ab0373c2badaaacee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming sequential credit data:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks:\n",
      "../data/train_data/train_data_0.pq\n",
      "../data/train_data/train_data_1.pq\n",
      "../data/train_data/train_data_2.pq\n",
      "../data/train_data/train_data_3.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd6e27dd75742c08cd1ce0bba042ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks:\n",
      "../data/train_data/train_data_4.pq\n",
      "../data/train_data/train_data_5.pq\n",
      "../data/train_data/train_data_6.pq\n",
      "../data/train_data/train_data_7.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ede58797b684b749a95ddfa84e11360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks:\n",
      "../data/train_data/train_data_8.pq\n",
      "../data/train_data/train_data_9.pq\n",
      "../data/train_data/train_data_10.pq\n",
      "../data/train_data/train_data_11.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793f1e5d0e7b49528679fdd2000f6308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 46s, sys: 5min 18s, total: 11min 5s\n",
      "Wall time: 10min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "aggregator = CountAggregator()\n",
    "train_data = aggregator.fit_transform(TRAIN_DATA_PATH, num_parts_to_preprocess_at_once=4, num_parts_total=12, \n",
    "                                      save_to_path=TRAIN_FEATURES_PATH, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объем оперативной памяти, занимаемой датафреймом с признаками: 1.683\n"
     ]
    }
   ],
   "source": [
    "print(f\"Объем оперативной памяти, занимаемой датафреймом с признаками: {train_data.memory_usage(index=True).sum() / 1e9}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Также извлечем признаки для заявок из тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:28: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b63bd4634c143038dac56ae1cfc36d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming sequential credit data:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks:\n",
      "../data/test_data/test_data_0.pq\n",
      "../data/test_data/test_data_1.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce776c30a934fa4bb83f0bdef562cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = aggregator.transform(TEST_DATA_PATH, num_parts_to_preprocess_at_once=2, num_parts_total=2,\n",
    "                                 save_to_path=TEST_FEATURES_PATH, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Model: LightGBM + CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для обучения модели помимо признаковых описаний нам также нужна целевая переменная. Целевая переменная для тренировочной выборки содержится в файле train_target.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TARGET_PATH = \"../data/train_target.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(TRAIN_TARGET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_target = train_target.merge(train_data, on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 421)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = list(train_data_target.columns.values)\n",
    "feature_cols.remove(\"id\"), feature_cols.remove(\"flag\")\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with fold 1 started\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.738614\n",
      "[100]\tvalid_0's auc: 0.747327\n",
      "[150]\tvalid_0's auc: 0.751475\n",
      "[200]\tvalid_0's auc: 0.753816\n",
      "[250]\tvalid_0's auc: 0.755322\n",
      "[300]\tvalid_0's auc: 0.756596\n",
      "[350]\tvalid_0's auc: 0.757394\n",
      "[400]\tvalid_0's auc: 0.758024\n",
      "[450]\tvalid_0's auc: 0.75856\n",
      "[500]\tvalid_0's auc: 0.759029\n",
      "[550]\tvalid_0's auc: 0.759293\n",
      "[600]\tvalid_0's auc: 0.759525\n",
      "[650]\tvalid_0's auc: 0.759801\n",
      "[700]\tvalid_0's auc: 0.760127\n",
      "[750]\tvalid_0's auc: 0.760259\n",
      "[800]\tvalid_0's auc: 0.760416\n",
      "[850]\tvalid_0's auc: 0.76063\n",
      "[900]\tvalid_0's auc: 0.760767\n",
      "[950]\tvalid_0's auc: 0.760866\n",
      "[1000]\tvalid_0's auc: 0.761047\n",
      "[1050]\tvalid_0's auc: 0.761213\n",
      "[1100]\tvalid_0's auc: 0.761264\n",
      "[1150]\tvalid_0's auc: 0.761389\n",
      "[1200]\tvalid_0's auc: 0.761439\n",
      "[1250]\tvalid_0's auc: 0.761575\n",
      "[1300]\tvalid_0's auc: 0.761673\n",
      "[1350]\tvalid_0's auc: 0.761724\n",
      "[1400]\tvalid_0's auc: 0.761776\n",
      "[1450]\tvalid_0's auc: 0.761836\n",
      "[1500]\tvalid_0's auc: 0.761903\n",
      "[1550]\tvalid_0's auc: 0.762027\n",
      "[1600]\tvalid_0's auc: 0.762072\n",
      "[1650]\tvalid_0's auc: 0.762107\n",
      "Early stopping, best iteration is:\n",
      "[1634]\tvalid_0's auc: 0.762117\n",
      "Training with fold 1 completed\n",
      "Training with fold 2 started\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.741671\n",
      "[100]\tvalid_0's auc: 0.750697\n",
      "[150]\tvalid_0's auc: 0.754911\n",
      "[200]\tvalid_0's auc: 0.757133\n",
      "[250]\tvalid_0's auc: 0.75852\n",
      "[300]\tvalid_0's auc: 0.759605\n",
      "[350]\tvalid_0's auc: 0.760348\n",
      "[400]\tvalid_0's auc: 0.760963\n",
      "[450]\tvalid_0's auc: 0.761476\n",
      "[500]\tvalid_0's auc: 0.761929\n",
      "[550]\tvalid_0's auc: 0.762146\n",
      "[600]\tvalid_0's auc: 0.762473\n",
      "[650]\tvalid_0's auc: 0.762746\n",
      "[700]\tvalid_0's auc: 0.762954\n",
      "[750]\tvalid_0's auc: 0.763144\n",
      "[800]\tvalid_0's auc: 0.763334\n",
      "[850]\tvalid_0's auc: 0.763553\n",
      "[900]\tvalid_0's auc: 0.763653\n",
      "[950]\tvalid_0's auc: 0.763812\n",
      "[1000]\tvalid_0's auc: 0.763897\n",
      "[1050]\tvalid_0's auc: 0.764026\n",
      "[1100]\tvalid_0's auc: 0.764086\n",
      "[1150]\tvalid_0's auc: 0.764197\n",
      "[1200]\tvalid_0's auc: 0.76423\n",
      "[1250]\tvalid_0's auc: 0.764328\n",
      "[1300]\tvalid_0's auc: 0.764431\n",
      "[1350]\tvalid_0's auc: 0.764528\n",
      "[1400]\tvalid_0's auc: 0.764578\n",
      "[1450]\tvalid_0's auc: 0.764623\n",
      "[1500]\tvalid_0's auc: 0.764621\n",
      "Early stopping, best iteration is:\n",
      "[1454]\tvalid_0's auc: 0.764645\n",
      "Training with fold 2 completed\n",
      "Training with fold 3 started\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.739963\n",
      "[100]\tvalid_0's auc: 0.748917\n",
      "[150]\tvalid_0's auc: 0.753193\n",
      "[200]\tvalid_0's auc: 0.755758\n",
      "[250]\tvalid_0's auc: 0.757121\n",
      "[300]\tvalid_0's auc: 0.758059\n",
      "[350]\tvalid_0's auc: 0.758917\n",
      "[400]\tvalid_0's auc: 0.759694\n",
      "[450]\tvalid_0's auc: 0.760303\n",
      "[500]\tvalid_0's auc: 0.760747\n",
      "[550]\tvalid_0's auc: 0.761026\n",
      "[600]\tvalid_0's auc: 0.761218\n",
      "[650]\tvalid_0's auc: 0.761419\n",
      "[700]\tvalid_0's auc: 0.761637\n",
      "[750]\tvalid_0's auc: 0.761739\n",
      "[800]\tvalid_0's auc: 0.761933\n",
      "[850]\tvalid_0's auc: 0.762124\n",
      "[900]\tvalid_0's auc: 0.762229\n",
      "[950]\tvalid_0's auc: 0.762344\n",
      "[1000]\tvalid_0's auc: 0.762469\n",
      "[1050]\tvalid_0's auc: 0.762482\n",
      "[1100]\tvalid_0's auc: 0.76258\n",
      "[1150]\tvalid_0's auc: 0.762608\n",
      "[1200]\tvalid_0's auc: 0.762603\n",
      "[1250]\tvalid_0's auc: 0.762677\n",
      "[1300]\tvalid_0's auc: 0.762745\n",
      "[1350]\tvalid_0's auc: 0.762856\n",
      "[1400]\tvalid_0's auc: 0.762857\n",
      "[1450]\tvalid_0's auc: 0.762867\n",
      "Early stopping, best iteration is:\n",
      "[1425]\tvalid_0's auc: 0.762911\n",
      "Training with fold 3 completed\n",
      "Training with fold 4 started\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.737814\n",
      "[100]\tvalid_0's auc: 0.747218\n",
      "[150]\tvalid_0's auc: 0.751475\n",
      "[200]\tvalid_0's auc: 0.753729\n",
      "[250]\tvalid_0's auc: 0.755373\n",
      "[300]\tvalid_0's auc: 0.756429\n",
      "[350]\tvalid_0's auc: 0.757444\n",
      "[400]\tvalid_0's auc: 0.758053\n",
      "[450]\tvalid_0's auc: 0.758583\n",
      "[500]\tvalid_0's auc: 0.759119\n",
      "[550]\tvalid_0's auc: 0.759556\n",
      "[600]\tvalid_0's auc: 0.75983\n",
      "[650]\tvalid_0's auc: 0.760018\n",
      "[700]\tvalid_0's auc: 0.7602\n",
      "[750]\tvalid_0's auc: 0.760417\n",
      "[800]\tvalid_0's auc: 0.76065\n",
      "[850]\tvalid_0's auc: 0.760721\n",
      "[900]\tvalid_0's auc: 0.760821\n",
      "[950]\tvalid_0's auc: 0.76095\n",
      "[1000]\tvalid_0's auc: 0.761015\n",
      "[1050]\tvalid_0's auc: 0.761189\n",
      "[1100]\tvalid_0's auc: 0.761366\n",
      "[1150]\tvalid_0's auc: 0.761412\n",
      "[1200]\tvalid_0's auc: 0.761469\n",
      "[1250]\tvalid_0's auc: 0.761533\n",
      "[1300]\tvalid_0's auc: 0.761605\n",
      "[1350]\tvalid_0's auc: 0.761752\n",
      "[1400]\tvalid_0's auc: 0.76176\n",
      "[1450]\tvalid_0's auc: 0.761775\n",
      "[1500]\tvalid_0's auc: 0.761805\n",
      "[1550]\tvalid_0's auc: 0.76183\n",
      "[1600]\tvalid_0's auc: 0.761882\n",
      "[1650]\tvalid_0's auc: 0.761924\n",
      "[1700]\tvalid_0's auc: 0.761967\n",
      "Early stopping, best iteration is:\n",
      "[1679]\tvalid_0's auc: 0.761997\n",
      "Training with fold 4 completed\n",
      "Training with fold 5 started\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.739714\n",
      "[100]\tvalid_0's auc: 0.748839\n",
      "[150]\tvalid_0's auc: 0.752872\n",
      "[200]\tvalid_0's auc: 0.755238\n",
      "[250]\tvalid_0's auc: 0.756871\n",
      "[300]\tvalid_0's auc: 0.757942\n",
      "[350]\tvalid_0's auc: 0.758795\n",
      "[400]\tvalid_0's auc: 0.759509\n",
      "[450]\tvalid_0's auc: 0.760166\n",
      "[500]\tvalid_0's auc: 0.760492\n",
      "[550]\tvalid_0's auc: 0.760843\n",
      "[600]\tvalid_0's auc: 0.761176\n",
      "[650]\tvalid_0's auc: 0.761473\n",
      "[700]\tvalid_0's auc: 0.761794\n",
      "[750]\tvalid_0's auc: 0.761929\n",
      "[800]\tvalid_0's auc: 0.762127\n",
      "[850]\tvalid_0's auc: 0.762251\n",
      "[900]\tvalid_0's auc: 0.762332\n",
      "[950]\tvalid_0's auc: 0.762486\n",
      "[1000]\tvalid_0's auc: 0.76257\n",
      "[1050]\tvalid_0's auc: 0.762643\n",
      "[1100]\tvalid_0's auc: 0.762761\n",
      "[1150]\tvalid_0's auc: 0.762879\n",
      "[1200]\tvalid_0's auc: 0.762971\n",
      "[1250]\tvalid_0's auc: 0.763088\n",
      "[1300]\tvalid_0's auc: 0.763191\n",
      "[1350]\tvalid_0's auc: 0.763264\n",
      "[1400]\tvalid_0's auc: 0.763285\n",
      "[1450]\tvalid_0's auc: 0.763373\n",
      "[1500]\tvalid_0's auc: 0.763394\n",
      "[1550]\tvalid_0's auc: 0.763465\n",
      "[1600]\tvalid_0's auc: 0.763569\n",
      "[1650]\tvalid_0's auc: 0.763592\n",
      "[1700]\tvalid_0's auc: 0.763619\n",
      "[1750]\tvalid_0's auc: 0.763642\n",
      "[1800]\tvalid_0's auc: 0.763641\n",
      "Early stopping, best iteration is:\n",
      "[1762]\tvalid_0's auc: 0.763668\n",
      "Training with fold 5 completed\n"
     ]
    }
   ],
   "source": [
    "targets = train_data_target[\"flag\"].values\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=100, shuffle=True)\n",
    "\n",
    "oof = np.zeros(len(train_data_target))\n",
    "train_preds = np.zeros(len(train_data_target))\n",
    "\n",
    "models = []\n",
    "\n",
    "tree_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 5,\n",
    "    \"reg_lambda\": 1,\n",
    "    \"num_leaves\": 64,\n",
    "    \"n_jobs\": 5,\n",
    "    \"n_estimators\": 2000\n",
    "}\n",
    "\n",
    "for fold_, (train_idx, val_idx) in enumerate(cv.split(train_data_target, targets), 1):\n",
    "    print(f\"Training with fold {fold_} started\")\n",
    "    lgb_model = lgb.LGBMClassifier(**tree_params)\n",
    "    train, val = train_data_target.iloc[train_idx], train_data_target.iloc[val_idx]\n",
    "    \n",
    "    lgb_model.fit(train[feature_cols], train.flag.values, eval_set=[(val[feature_cols], val.flag.values)],\n",
    "              early_stopping_rounds=50, verbose=50)\n",
    "\n",
    "    oof[val_idx] = lgb_model.predict_proba(val[feature_cols])[:, 1]\n",
    "    train_preds[train_idx] += lgb_model.predict_proba(train[feature_cols])[:, 1] / (cv.n_splits-1)\n",
    "    models.append(lgb_model)\n",
    "    print(f\"Training with fold {fold_} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train roc-auc:  0.8031990015398422\n"
     ]
    }
   ],
   "source": [
    "print(\"Train roc-auc: \", roc_auc_score(targets, train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV roc-auc:  0.763063554599203\n"
     ]
    }
   ],
   "source": [
    "print(\"CV roc-auc: \", roc_auc_score(targets, oof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Подготовим посылку в проверяющую систему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e627070e154897ab227ffc043c7bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score = np.zeros(len(test_data))\n",
    "\n",
    "for model in tqdm.tqdm_notebook(models):\n",
    "    score += model.predict_proba(test_data[feature_cols])[:, 1] / len(models)\n",
    "    \n",
    "submission = pd.DataFrame({\n",
    "    \"id\" : test_data[\"id\"].values,\n",
    "    \"score\": score\n",
    "}) \n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=None) # ~ 0.795 roc-auc на public test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
